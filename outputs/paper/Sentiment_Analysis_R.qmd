---
title: "Sentiment Analysis of User Comments on Reddit Posts: A Data-driven Approach Using R"
author: "Joseph Chung"
date: today
date-format: "MMM D, YYYY"
format:
  pdf:
    documentclass: article
    geometry: margin = 1in
    abstract: This paper presents an in-depth analysis of the growth, engagement, and sentiment trends in the fields of Machine Learning, Artificial Intelligence, and Data Science based on post activity and discussions on a popular platform. Using various visualization techniques, sentiment analysis models, and topic modeling approaches like Latent Dirichlet Allocation (LDA), the study uncovers the dynamics, interconnectedness, and sentiments associated with these fields. The findings reveal a significant increase in post activity, distinct words per post, and predominantly positive sentiment, indicating growing interest, understanding, and appreciation for the potential applications and implications of these technologies. Furthermore, the study highlights the synergistic relationship between data science, artificial intelligence, and deep learning, emphasizing the importance of collaboration and interdisciplinary research for driving innovation and unlocking new potential applications. Despite some limitations, such as data source biases and potential inaccuracies in sentiment and topic modeling, the results provide valuable insights into the evolving landscape of Machine Learning, Artificial Intelligence, and Data Science, and their impact on various industries and the job market.
    toc: true
    number-sections: true
    colorlinks: true
    lof : true
    lot : true
thanks: "Code and data are available at: https://github.com/UtopianYoungChung/Sentiment-Analysis-Using-R.git"
bibliography: bibliography.bib
---

```{r}
#| messages: false
#| echo: false
#| warning: false

# Load required packages and read in the data
knitr::opts_chunk$set(echo = TRUE)
library(dplyr) #data manipulation
library(ggplot2) #visualizations
library(gridExtra) #viewing multiple plots together
library(tidyverse) #lexicon, sentiments
library(stringr) #lexicon, sentiments
library(tidytext) #text mining
library(wordcloud2) #creative visualizations
library(kableExtra)
library(formattable)
library(tidyr) #Spread, separate, unite, text mining
library(widyr) #Use for pairwise correlation
library(furrr)
library(SnowballC)
library(vctrs)
library(slider)
library(magrittr)
library(purrr)
library(lubridate)
library(quanteda)
library(stm)

#Visualizations!
library(ggplot2) #Visualizations (also included in the tidyverse package)
library(ggrepel) #`geom_label_repel`
library(gridExtra) #`grid.arrange()` for multi-graphs
library(knitr) #Create nicely formatted output tables
library(kableExtra) #Create nicely formatted output tables
library(formattable) #For the color_tile function
library(circlize) #Visualizations - chord diagram
library(memery) #Memes - images with plots
library(magick) #Memes - images with plots (image_read)
library(yarrr)  #Pirate plot
library(radarchart) #Visualizations
library(igraph) #ngram network diagrams
library(ggraph) #ngram network diagrams

#Define some colors to use throughout
my_colors <- c("#E69F00", "#56B4E9", "#009E73", "#CC79A7", "#D55E00", "#D65E00")

#Customize ggplot2's default theme settings
theme_posts <- function(aticks = element_blank(),
                        pgminor = element_blank(),
                        lt = element_blank(),
                        lp = "none")
{
  theme(plot.title = element_text(hjust = 0.5), #Center the title
        axis.ticks = aticks, #Set axis ticks to on or off
        panel.grid.minor = pgminor, #Turn the minor grid lines on or off
        legend.title = lt, #Turn the legend title on or off
        legend.position = lp) #Turn the legend on or off
}

#Customize the text tables for consistency using HTML formatting
my_kable_styling <- function(dat, caption) {
  kable(dat, "html", escape = FALSE, caption = caption) %>%
    kable_styling(bootstrap_options = c("striped", "condensed", "bordered"),
                  full_width = FALSE)
}

```

# Introduction

The rapid growth of online forums and communities discussing various topics, including Machine Learning(ML), has led to an abundance of user-generated content in the form of text data. Analyzing this data can reveal valuable insights into user perceptions, opinions, and sentiments toward specific subjects. Sentiment analysis, or opinion mining, is a natural language processing technique used to determine the sentiment expressed in a piece of text, such as positive, negative, or neutral. This paper focuses on employing sentiment analysis techniques to understand the sentiment polarity of user comments related to the topics of Machine Learning, Data Science, and Artificial Intelligence.

## Background and motivation

Machine Learning, Data Science, and Artificial Intelligence have become popular fields of study and research due to their vast applications in various industries. As a result, online communities have emerged where people discuss these topics, share knowledge, and express their opinions. Analyzing the sentiment of these users' comments can provide insights into the perception of the Data Science community, identify common issues or concerns, and uncover emerging trends. This understanding could be valuable for researchers, educators, and industry professionals who aim to improve their work based on community feedback.

## Research questions

This study aims to address the following research questions:

1.  What is the overall sentiment polarity (positive, negative, or neutral) of user comments related to Machine Learning, Data Science, and Artificial Intelligence?

2.  How is the sentiment polarity distributed across different posts in the dataset?

3.  Are there any correlations between post attributes (e.g., post length, engagement) and sentiment polarity?

4.  Can any trends or patterns in sentiment polarity be identified over time or in relation to specific subtopics within the topics of Machine Learning, Data Science, or Artificial Intelligence?

## Scope and limitations

The scope of this study is limited to the analysis of a dataset containing post \_ids and comments of the users related to the topics. The results and interpretations are based solely on the data provided and may not be generalizable to other online communities or topics. The sentiment analysis model employed in this study is subject to the limitations inherent in natural languages processing techniques, such as context ambiguity and handling of sarcasm or irony. Additionally, the study does not account for demographic information or biases present in the dataset, which could potentially influence the sentiment analysis results.

# Literature Review

## Sentiment analysis in online communities

Over the past decade, sentiment analysis has become increasingly popular as a tool to mine user opinions and emotions from online platforms, such as social media, blogs, and forums [@citePang]. Numerous studies have focused on the application of sentiment analysis in various domains, such as politics [@citeTumasjan], finance [@citeBollen], and consumer reviews [@citeDave]. These studies demonstrate the potential of sentiment analysis as a valuable technique for understanding user-generated content and informing decision-making processes.

## Sentiment analysis techniques in R

R is a widely used programming language for statistical analysis and data visualization, with several libraries and packages available for natural language processing and sentiment analysis [@citeR]. Some of the most popular sentiment analysis packages in R include tidytext [@citeSilge], and syuzhet [@citeJockers]. These packages provide various algorithms and techniques for sentiment analysis, such as lexicon-based approaches, machine-learning models, and deep-learning methods. Researchers have employed these techniques to analyze sentiment in diverse contexts, ranging from Twitter data [@citeWijeratne] to news articles [@citeKearney]

## Applications of sentiment analysis in Machine Learning research

Sentiment analysis has been employed in Machine Learning research to understand user perceptions, opinions, and trends related to the field. For example, Aliza [@citeAliza] analyzed the sentiment of tweets related to Machine Learning conferences, revealing insights into the community's responses to specific presentations and events. In another study, Wang [@citeWang] investigated the sentiment of user comments on popular Machine Learning platforms, such as GitHub and Stack Overflow, to identify trends and patterns in the community's engagement with specific algorithms and techniques. These studies highlight the potential of sentiment analysis to uncover valuable insights into the Machine Learning community and inform future research directions.

The literature demonstrates the growing interest in sentiment analysis as a valuable tool for understanding user-generated content in various domains. R has emerged as a popular platform for sentiment analysis due to its extensive libraries and packages. However, there remains a need for more comprehensive studies that investigate the sentiment of user comments related to Machine Learning, particularly in the context of online communities where users share their knowledge and opinions on the topic. This study aims to address this gap by employing sentiment analysis techniques in R to analyze a dataset of post IDs and comments related to Machine Learning on the Reddit platform.

# Data Collection and Preprocessing

This section describes the process of data collection, including web scraping and API requests, and the subsequent preprocessing steps applied to the dataset to prepare it for sentiment analysis.

## Data Collection

The dataset used in this study was collected from Reddit, a popular online platform where users can share and discuss a wide range of topics. The data was obtained by performing web scraping and API requests on the topics of Machine Learning, Data Science, and Artificial Intelligence posts between 2013 to 2023. The Reddit API (https://www.reddit.com/dev/api/) was utilized to access and retrieve relevant posts and associated comments from the specified subreddits (e.g., r/MachineLearning, r/datascience, r/artificial).

To perform the web scraping and API requests, the Python package PRAW (Python Reddit API Wrapper) [@citePRAW] was used, which provides a convenient interface to interact with the Reddit API. The data collection process involved:

1.  Authenticating with the Reddit API using a registered application's credentials.

2.  Querying the API for posts related to the specified topics (Machine Learning, Data Science, and Artificial Intelligence) within the targeted subreddits.

3.  Combining the collected data into a structured dataset for further analysis.

```{r}
#| messages: false
#| echo: false
#| warning: false
#| label: tbl-table1
#| tbl-cap: Ten observations from dataset of Reddit API
#Load in dataset
reddit_raw <- read.csv(here::here("inputs/data/DS_ML_AI_posts.csv"))

#Extracting random ten observations from the Reddit dataset
reddit_raw %>%
  slice_sample(n = 10) %>%
  select(post_id, subreddit, score, created_year) %>%
  kable("pipe", escape = FALSE, align = "c", caption = "10 observaions") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)
  
```

@tbl-table1 shows ten observations of the dataset, with 4 selected attributes of 13. The data types of 13 attributes are explained below:

-   post_id: VARCHAR

-   subreddit: CHAR

-   created_utc: INT

-   selftext: CHAR

-   post_url: VARCHAR

-   post_title: VARCHAR

-   link_flair_text: CHAR

-   score: INT

-   num_coments: INT

-   upvote_ratio: FLOAT

-   created_date: DATETIME

-   created_year: YEAR

-   comment: VARCHAR

It contains 223,781 observations and 13 variables in total.
\newpage

## Data cleaning and preprocessing

After obtaining the raw dataset, several preprocessing steps were performed to clean and prepare the data for sentiment analysis. These steps include:

### Basic cleaning

There are different methods we can use to condition the data, but this paper will stick to the basics and use gsub() and apply() functions to do the cleaning [@citeBecker].

First, we got rid of those pesky contractions by creating a little function that handles most scenarios using gsub(), and then applied that function across all comments. Second, all those special characters that muddy the text were removed with gsub() function and a simple regular expression. Lastly, to be consistent, we converted everything to lowercase with tolower() function.

All the work was done on a separate R file called 02-data_preprocessing.R. It can be found in the 'script' folder.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-table2
#| tbl-cap: 10 Random observations were drawn from cleaned dataset from Reddit API
reddit <- read.csv(here::here("inputs/data/reddit.csv"))

#Extracting random ten observations from the Reddit dataset
reddit %>%
  filter(nchar(comment) < 30) %>%
  slice_sample(n = 10) %>%
  select(post_id, comment) %>%
  kable("pipe", escape = FALSE, align = "c", caption = "10  Random observaions") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)

```

### Text Mining

Text mining and text analytics can be used interchangeably. The primary objective is to uncover significant data that may be concealed or unfamiliar beneath the surface. One of the techniques utilized in text mining is Natural Language Processing (NLP), which aims to unravel the intricacies in written language through various methods such as tokenization, clustering, entity extraction, and analyzing the relationships between words. Furthermore, NLP employs algorithms to detect patterns and quantify subjective data.

### Tokenization

To unnest the tokens, we use the tidytext library [@citeTidytxt]. From the tidy text framework, we broke the text into individual tokens and transform it into a tidy data structure. To do this, we used tidytext's unnest_tokens() function. unnest_tokens() requires at least two arguments: the output column name that will be created as the text is unnested into it ("word," in this case) and the input column that holds the current text (i.e. comments). We then took the Reddit dataset and pipe it into unnest_tokens() and then removed stop words. There are different lists to choose from, but here we used the lexicon called stop_words from the tidytext package.

```{r}
#| messages: false
#| echo: false
#| warning: false
#unnest and remove stop, and short words


reddit_comments_filtered <- reddit %>%
  unnest_tokens(word, comment) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(nchar(word) > 3) %>% # remove words with characters less than 3
  filter(!grepl('[0-9]', word)) # remove number


#class(reddit_comments_filtered)
#dim(reddit_comments_filtered)
```

After the tokenization, we used dplyr's anti_join() [@citeDplyr] to remove stop words. Then we used distinct() to get rid of any duplicate records as well. Lastly, we examined the class and dimensions of our new tidy data structure 'reddit_comments_filtered' is a data frame with 2914188 total words (not unique words) and 8 columns. The following is the snapshot with chosen token 'deepmind':

```{r}
#| messages: false
#| echo: false
#| warning: false
#| label: tbl-table3
#| tbl-cap: Tokenized Format

reddit_comments_filtered %>%
  filter(word == "deepmind") %>%
  select(word, post_id, subreddit, score, years) %>%
  arrange() %>%
  distinct() %>%
  top_n(10, post_id) %>%
  mutate(post_id = color_tile("lightblue","lightblue")(post_id)) %>%
  mutate(word = color_tile("lightgreen","lightgreen")(word)) %>%
  kable("simple", escape = FALSE, align = "c", caption = "Tokenized Format Example") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)
```

@tbl-table3 shows us the tokenized, unsummarized, tidy data structure. \newpage

# Descriptive Statistics

During this section, we will be using creative graphs from the ggplot2 [@citeggplot2], circlize [@citecirclize], and yarrr [@citeyarrr] packages.

## Shipshape: Word Count Per Post

The term "shipshape" is commonly used to indicate that everything is well-organized, neat, and tidy. In this context, an interesting perspective is presented on the orderly and organized data that demonstrates the lexical diversity, or the range of vocabulary, found within post comments over time. The pirate plot, a sophisticated technique for graphing a continuous dependent variable (such as word count) against a categorical independent variable (such as 'year'), is utilized to create a comprehensive and informative visual representation that incorporates both raw data points and statistical analysis.

```{r}
#| messages: false
#| echo: false
#| warning: false
#| label: fig-graph1
#| fig-cap: Lexical Diversity over the past 10 years, between '13 and '23
word_summary <- reddit_comments_filtered %>%
  group_by(years, post_id) %>%
  mutate(word_count = n_distinct(word)) %>%
  select(Years = years, Post = post_id, subreddit, word_count) %>%
  distinct() %>% #To obtain one record per post
  ungroup()

pirateplot(formula =  word_count ~ Years + subreddit, #Formula
   data = word_summary, #Data frame
   xlab = NULL, ylab = "Post Distinct Word Count", #Axis labels
   main = "Lexical Diversity between Year 2013 and 2023", #Plot title
   pal = "google", #Color scheme
   point.o = .2, #Points
   avg.line.o = 1, #Turn on the Average/Mean line
   theme = 0, #Theme
   point.pch = 16, #Point `pch` type
   point.cex = 1.5, #Point size
   jitter.val = .1, #Turn on jitter to see the songs better
   cex.lab = .9, cex.names = .7) #Axis label size


```

In this pirate plot displayed in figure @fig-graph1, each colored circle symbolizes a post. The dense clusters within each category indicate a considerable volume of posts in the dataset. Upon observation, it becomes evident that there were only a few posts during the period of '13-15 across all topics, whereas from '16-18 onwards, there was a surge in post activity. Additionally, it is worth noting that there is a slight upward trend in the number of distinct words per post in the Machine Learning topic. The solid horizontal line represents the mean word count for the corresponding years. \newpage

## Assumption Checking and Methods

### All Year Round: Post Count Per Year

```{r}
#| messages: false
#| echo: false
#| warning: false
#| label: fig-bar2
#| fig-cap: The total amount of posts on Reddit platform between Year '13 and '23


posts_year <- reddit_comments_filtered %>%
  select(category, created_year) %>%
  distinct() %>%
  group_by(created_year) %>%
  summarise(post_count = n())

id <- seq_len(nrow(posts_year))
posts_year <- cbind(posts_year, id)
label_data = posts_year
number_of_bar = nrow(label_data) #Calculate the ANGLE of the labels
angle = 90 - 360 * (label_data$id - 0.5) / number_of_bar #Center things
label_data$hjust <- ifelse(angle < -90, 1, 0) #Align label
label_data$angle <- ifelse(angle < -90, angle + 180, angle) #Flip angle
ggplot(posts_year, aes(x = as.factor(id), y = post_count)) +
  geom_bar(stat = "identity", fill = alpha("purple", 0.7)) +
  geom_text(data = label_data, aes(x = id, y = post_count + 10, label = created_year, hjust = hjust), color = "black", alpha = 0.6, size = 3, angle =  label_data$angle, inherit.aes = FALSE ) +
  coord_polar(start = 0) +
  ylim(-20, 125) + #Size of the circle
  theme_minimal() +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        panel.grid = element_blank(),
        plot.margin = unit(rep(-4,4), "in"),
        plot.title = element_text(margin = margin(t = 10, b = -10)))


```

@fig-bar2 depicts the total number of posts in the areas of 'Machine Learning', 'Artificial Intelligence', and 'Data Science' from 2019 to 2022. During this time, there were many advancements in artificial intelligence, including the development of Stable Diffusion (2022), DALL-E (2021) ChatGPT (2022) and other important breakthroughs. Now that we are in 2023, the number of posts has already grown a lot compared to previous years. This suggests that people are more active in these topics than they were in the last 10 years. \newpage

### Chords: Subreddit posts by Years

```{r}
#| messages: false
#| echo: false
#| warning: false
#| label: fig-bar3
#| fig-cap: Relathiopship between Subreddit posts and timeline in semi-annual frames


years_subreddit <-  reddit_comments_filtered %>%
  filter(years != "NA") %>% #Remove posts without posted dates
  count(years, subreddit)  #Get post count per subreddit per semi-annual. Order determines top or bottom.

circos.clear() #Very important - Reset the circular layout parameters!
grid.col = c("13-15" = my_colors[1], "16-18" = my_colors[2], "19-21" = my_colors[3], "22-23" = my_colors[4], "Subreddit" = "grey", "Uncharted" = "grey") #assign chord colors
# Set the global parameters for the circular layout. Specifically the gap size
circos.par(gap.after = c(rep(5, length(unique(years_subreddit[[1]])) - 1), 15,
                         rep(5, length(unique(years_subreddit[[2]])) - 1), 15))

chordDiagram(years_subreddit, grid.col = grid.col, transparency = .2)
title("Relationship Between Subreddit and Timeline")


```

As shown above in @fig-bar3, the subject Machine Learning is experienced significant growth between 2019 and 2021, and this trend has persisted into 2023.

## Sentiment analysis model selection

### Explore sentiment lexicons

The tidytext [@citeTidytxt] package includes a dataset called sentiments which provides several distinct lexicons. These lexicons are dictionaries of words with an assigned sentiment category or value. Tidytext provides three general purposes lexicons:

-   AFINN: assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment

-   Bing: assigns words into positive and negative categories

-   NRC: assigns words into one or more of the following ten categories: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust

In order to examine the lexicons, we create a data frames. \newpage

#### Create sentiment datasets

Start off by creating Post sentiment datasets for each of the lexicons by performing an inner_join() on the get_sentiments() function. We pass the name of the lexicon for each call. For this paper, we use Bing for binary and NRC for categorical sentiments. Since words can appear in multiple categories in NRC, such as Negative/Fear or Positive/Joy, we will also create a subset without the positive and negative categories to use later on. Refer to 03-simulation.R

```{r}
#| message: false
#| echo: false
#| warning: false
#### Create sentiment datasets
post_bing <- reddit_comments_filtered %>%
  inner_join(get_sentiments("bing"))

post_nrc <- reddit_comments_filtered %>%
  inner_join(get_sentiments("nrc"))

post_nrc_sub <- reddit_comments_filtered %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(!sentiment %in% c("positive", "negative"))

```

#### Mood changes over time

Since we are looking at sentiment from a polarity perspective, we might want to see whether or not it changes over time. We use geom_smooth() [@citeggplot2] with loses method for a smooth curve and another geom_smooth() with method=lm for a linear smooth curve.

```{r}
#| warning: false
#| echo: false
#| message: false
#| label: fig-bar4
#| fig-cap: Sentiment ploarity over time and Percet postivie over time are descreasing


post_polarity_year <- post_bing %>%
  count(sentiment, created_year) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(polarity = positive - negative,
    percent_positive = positive / (positive + negative) * 100)

polarity_over_time <- post_polarity_year %>%
  ggplot(aes(created_year, polarity, color = ifelse(polarity >= 0,my_colors[5],my_colors[4]))) +
  geom_col() +
  geom_smooth(method = "loess", se = FALSE) +
  geom_smooth(method = "lm", se = FALSE, aes(color = my_colors[1])) +
  theme_posts() + theme(plot.title = element_text(size = 11)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Polarity Over Time")

relative_polarity_over_time <- post_polarity_year %>%
  ggplot(aes(created_year, percent_positive , color = ifelse(polarity >= 0,my_colors[5],my_colors[4]))) +
  geom_col() +
  geom_smooth(method = "loess", se = FALSE) +
  geom_smooth(method = "lm", se = FALSE, aes(color = my_colors[1])) +
  theme_posts() + theme(plot.title = element_text(size = 11)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Percent Positive Over Time")

grid.arrange(polarity_over_time, relative_polarity_over_time, ncol = 2)


```

@fig-bar4 shows that the overall polarity trend over time is negative in both cases.

#### Sentiment in Machine Learning

Let's take a deeper look into the mood of a specific topic over time: Machine Learning

How would NRC model interpret the mood of posts under Machine Learning? \newpage

```{r}
#| message: false
#| warning: false
#| echo: false
#| label: fig-bar5
#| fig-cap: The interpretation of NRC model in mood of posts under Machine Learning

post_nrc %>%
  filter(subreddit %in% "MachineLearning") %>%
  filter(category %in% "Discussion") %>%
  filter(years %in% "19-21") %>%
  group_by(sentiment) %>%
  summarise(word_count = n()) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, word_count)) %>%
  ggplot(aes(sentiment, word_count, fill = -word_count)) +
  geom_col() +
  guides(fill = FALSE) +
  theme_minimal() + theme_posts() +
  labs(x = NULL, y = "Word Count") +
  ggtitle("Machine Learning NRC Sentiment") +
  coord_flip()

```

Although highly subjective, the results in @fig-bar5 appear to confirm that there is overwhelmingly positive and trust sentiment with Machine Learning. Let's go further in depth by looking at the sentiment categories with distinctive topics ('Career' and 'discussion') and see if they appear to be correlated.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-bar6
#| fig-cap: NRC sentiment showing the level of anticipation is increasing over time, under 'Career'

post_nrc_sub %>%
  filter(category %in% c("Career")) %>%
  count(category, sentiment, created_year) %>%
  mutate(sentiment = reorder(sentiment, n), category = reorder(category, n)) %>%
  ggplot(aes(sentiment, n, fill = sentiment)) +
  geom_col() +
  facet_wrap(created_year ~ category, scales = "free_x", labeller = label_both) +
  theme_posts() +
  theme(panel.grid.major.x = element_blank(),
        axis.text.x = element_blank()) +
  labs(x = NULL, y = NULL) +
  ggtitle("NRC Sentiment Subreddit Analysis") +
  coord_flip()


```

Sub-NRC sentiments, @fig-bar6, show that anticipation is increasing under career category over time. Does this tell us that people are worrying about their jobs?

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-bar7
#| fig-cap: Bing sentiment showing the level of anticipation increasing over time, under 'Disscussion'

library(reshape2)
library(wordcloud)
post_bing %>%
  #filter((category %in% c("career"))) %>%
  #inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "dark green"), max.words = 100)

```

Bing sentiments, @fig-bar7, show many positive words from the posts under "discussion' category. Here, we can notice that some of the negative words included 'hard', 'wrong', and 'issue'.
\newpage

## Model training and validation

### Word embeddings

Word embeddings are a way to represent text data as vectors of numbers based on a huge corpus of text, capturing semantic meaning from words' context. First, let's filter out words that are used only rarely in this data set and create a nested dataframe with one row per post.

Next, we create a slide_windows() function using the slide() function from the slider package [@citeslider]. This new function identifies skipgram windows in order to calculate the skipgram probabilities, how often we find each word near each other word.

Now we can find all the skipgram windows; let's calculate how often words occur on their own, and how often words occur together with other words. We do this using point-wise mutual information (PMI) [@citePMI].

In statistics, probability theory and information theory, pointwise mutual information (PMI), or point mutual information, is a measure of association. It compares the probability of two events occurring together to what this probability would be if the events were independent.

$$pmi(x;y) \equiv \log_2 \frac{p(x,y)}{p(x)p(y)} = \log_2 \frac{p(x|y)}{p(x)}=\log_2 \frac{p(y|x)}{p(y)}$$

It's the logarithm of the probability of finding two words together, normalized for the probability of finding each of the words alone. We use PMI to measure which words occur together more often than expected based on how often they occur on their own. This step is the computationally expensive part of finding word embeddings. However, by using furrr package [@citefurrr], we can take advantage of parallel processing because identifying skipgram windows in one document is independent of all the other documents.

When PMI is high, the two words are associated with each other, i.e., likely to occur together. When PMI is low, the two words are not associated with each other, unlikely to occur together.

We then determine the word vectors from the PMI values using singular value decomposition (SVD). SVD is a method for dimensionality reduction via matrix factorization [@citeGolub] that works by taking our data and decomposing it onto special orthogonal axes.

In our application, we will use SVD to factor the PMI matrix into a set of smaller matrices containing the word embeddings with a size we get to choose. We will be using the widely_svd() function in widyr [@citewidyr], creating 100-dimensional word embeddings.

We now have our Reddit word embeddings.

## Exploring Reddit word embeddings

Now that we have determined word embeddings for the data set of Reddit posts, let's explore them and discuss how they are used in modelling.

Each word can be represented as a numeric vector in the new set of 100-dimensional feature spaces. A single word is mapped to only one vector, therefore, all senses of a word are conflated in word embeddings. Because of this, word embeddings are limited to understanding lexical semantics.

In order to find out which words are close to each other, we created a simple function that will find the nearest words to any given examples using our newly created word embeddings. This function takes the tidy word embeddings as input, along with a word/token as a string. Let's explore what words are closest to "MachineLearning" in the data set of Reddit posts, as determined by our word embeddings. The results are shown on @tbl-table4

```{r}
#| message: false
#| echo: false
#| warning: false
# Word Embedding
tidy_reddit <- reddit %>%
  select(post_id, comment) %>%
  unnest_tokens(word, comment) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(nchar(word) > 3) %>% # remove words with characters less than 3
  filter(!grepl('[0-9]', word)) %>% # remove numbers
  add_count(word) %>%
  filter(n >= 50) %>%
  select(-n)

nested_words <- tidy_reddit %>%
  nest(words = c(word))

# slide windows
slide_windows <- function(tbl, window_size) {
  skipgrams <- slider::slide(
    tbl, 
    ~.x, 
    .after = window_size - 1, 
    .step = 1, 
    .complete = TRUE
  )
  
  safe_mutate <- safely(mutate)
  
  out <- map2(skipgrams,
              1:length(skipgrams),
              ~ safe_mutate(.x, window_id = .y))
  
  out %>%
    transpose() %>%
    pluck("result") %>%
    compact() %>%
    bind_rows()
}

# PMI
plan(multisession)  ## for parallel processing

tidy_pmi <- nested_words %>%
  mutate(words = future_map(words, slide_windows, 4L)) %>%
  unnest(words) %>%
  unite(window_id, post_id, window_id) %>%
  pairwise_pmi(word, window_id)

# Tidyword vectors
tidy_word_vectors <- tidy_pmi %>%
  widely_svd(
    item1, item2, pmi,
    nv = 100, maxit = 1000
  )

# Function for...
nearest_neighbors <- function(df, token) {
  df %>%
    widely(
      ~ {
        y <- .[rep(token, nrow(.)), ]
        res <- rowSums(. * y) / 
          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))
        
        matrix(res, ncol = 1, dimnames = list(x = names(res)))
      },
      sort = TRUE
    )(item1, dimension, value) %>%
    select(-item2)
}

```

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: tbl-table4
#| tbl-cap: Top 10 related tokens with their probabilities to 'Machine Learning'
tidy_word_vectors %>%
  nearest_neighbors("machinelearning") %>%
  select(item1, value) %>%
  arrange() %>%
  top_n(10, value) %>%
  mutate(item1 = color_tile("lightblue","lightblue")(item1)) %>%
  mutate(value = color_tile("lightgreen","lightgreen")(value)) %>%
  kable("simple", escape = FALSE, align = "c", caption = "Tokenized Format Example") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                  full_width = FALSE)
```

Since we have found word embeddings via singular value decomposition, we can use these vectors to understand what principal components explain the most variation in the Reddit posts.

In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix [@citeSVD]. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any $m \times n$ matrix. It is related to polar decomposition. Specifically, the singular value decomposition of an $m \times n$ complex matrix is the factorization of the form: $$ M=U\Sigma V $$ , where U is an $m \times m$ complex unitary matrix, \$ \Sigma \$ is an \$ m \times n \$ rectangular diagonal matrix with non-negative real numbers on the diagonal, V is an \$ n \times n \$ complex unitary matrix, and V\* is the conjugate transpose of V. Such decomposition always exists for any complex matrix. If M is real, then U and V can be guaranteed to be real orthogonal matrices; in such contexts, the SVD is often denoted: $$ U \Sigma V^T $$ The orthogonal axes that svd used to represent our data were chosen so that the first axis accounts for the most variance, the second axis accounts for the next most variance, and so on. We can now explore which and how much each original tokens contributed to each of the resulting principal components produced using SVD.

```{r}
#| message: false
#| echo: false
#| warning: false
#| label: fig-bar8
#| fig-cap: The produced principal components using SVD show tops words being used in the posts
#| fig.width: 10
#| fig.height: 10 

tidy_word_vectors %>%
  filter(dimension <= 8) %>%
  group_by(dimension) %>%
  top_n(12, abs(value)) %>%
  ungroup() %>%
  mutate(item1 = reorder_within(item1, value, dimension)) %>%
  ggplot(aes(item1, value, fill = dimension)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~dimension, scales = "free_y", ncol = 4) +
  scale_x_reordered() +
  coord_flip() +
  labs(
    x = NULL,
    y = "Value",
    title = "First 8 principal components for text of Reddit post",
    subtitle = paste("Top words contributing to the components that explain",
                     "the most variation")
  )

```

@fig-bar8 shows some interesting aspects of the relationships in how very common words are used together. For instance, component 6 shows us how 'statistics', 'algebra', 'mathematics' are often used together; component 3 shows 'vectors', 'parameters', and 'gradient' are often used together. Considering how these words are related to data science, artificial intelligence, and deep learning, this results are interesting.

\newpage

## LDA Model: 'Text as Data'

Latent Dirichlet Allocation (LDA) is a generative probabilistic model used in natural language processing and text analysis for uncovering hidden topics in a collection of documents. Introduced by David Blei, Andrew Ng, and Michael I. Jordan in 2003, LDA is a type of unsupervised machine learning algorithm that helps identify underlying structures in the data without any prior information about the documents' content \[@citeLDA}.

LDA is based on the assumption that documents are composed of a mixture of topics, and each topic is a probability distribution over a fixed vocabulary. In other words, LDA assumes that the words in a document are generated by a combination of latent topics, where each topic is represented by a distribution of words. The aim of LDA is to learn the latent topics and their distribution across documents.

To implement LDA, the following steps are typically followed:

1.  Determine the number of topics (K) that you believe exist within the document collection.
2.  Randomly assign each word in each document to one of the K topics.
3.  For each document, update the topic assignments for its words by considering:
    a.  The overall prevalence of each topic in the document (topic-document distribution).
    b.  The overall prevalence of the word across all topics (word-topic distribution).
4.  Iterate through step 3 multiple times, updating the topic assignments until they converge or a predefined stopping criterion is met.

After the model converges, you can interpret the topics by examining the most probable words associated with each topic. Additionally, you can compute the distribution of topics for each document, providing insights into the main themes present in the document collection.

It's important to note that LDA has some limitations:

-   It assumes that the order of the words in the documents does not matter (bag-of-words assumption), which may not hold true in all cases.
-   The choice of the number of topics (K) can significantly impact the results, and selecting the optimal K can be challenging.
-   LDA may not always provide clear and interpretable topics, depending on the dataset and parameter settings.

Despite these limitations, LDA remains a widely used and powerful technique for topic modeling and extracting valuable insights from large text collections.

The subsequent activity is adapted from a chapter titled "Text as Data" in the book "Telling Stories with Data" by Rohan Alexander [@citeRohan].

```{r}
#| message: false
#| echo: false
#| warning: false

reddit_sentiment <- read.csv(here::here("inputs/data/reddit.csv"))


reddit_sentiment_corpus <-
  corpus(reddit_sentiment, docid_field = "X", text_field = "comment")



reddit_dfm <-
  reddit_sentiment_corpus |>
  tokens(
    remove_punct = TRUE,
    remove_symbols = TRUE
  ) |>
  dfm() |>
  dfm_trim(min_termfreq = 2, min_docfreq = 2) |>
  dfm_remove(stopwords(source = "snowball"))
```

```{r}
#| message: false
#| echo: false
#| warning: false


# LDM model
reddit_topics <- stm(documents = reddit_dfm, K = 3)

write_rds(
  reddit_topics,
  file = "reddit_topics.rda"
)

labelTopics(reddit_topics) 

```

In light of the findings presented above, it is evident that numerous positive discussions related to the subjects: 'Deep Learning', 'Data Science', and 'Artificial Intelligence' have taken place on the platform. These results corroborate the conclusions drawn from previous sentiment analyses.

# Discussion

## Interpretation of results

The pirate plot in figure @fig-graph1 reveals a significant increase in post activity in the areas of Machine Learning, Artificial Intelligence, and Data Science between 2016 and 2018 compared to the period of 2013-2015. This surge in post activity is indicative of the growing interest and advancements in these fields.

The slight upward trend in the number of distinct words per post in the Machine Learning topic suggests that discussions are becoming more in-depth and diverse. Furthermore, the increase in total number of posts from 2019 to 2022 as shown in @fig-bar2 highlights the continued growth in these topics.

The upward trend in Machine Learning, as seen in @fig-bar3, is evidence of the significant growth experienced in the field between 2019 and 2021, which has persisted into 2023. Despite the overall negative polarity trend observed in @fig-bar4, @fig-bar5 shows that there is a predominantly positive and trust sentiment associated with Machine Learning.

Delving deeper into sentiment categories within specific topics like 'Career' and 'Discussion', we find interesting correlations. For instance, the increasing anticipation under the career category over time, as shown in @fig-bar6, might indicate that people are becoming more concerned about their jobs in the face of advancing technology. On the other hand, Bing sentiments in @fig-bar7 reveal many positive words within the 'Discussion' category, although some negative words like 'hard', 'wrong', and 'issue' are also present.

@fig-bar8 illustrates how common words related to data science, artificial intelligence, and deep learning are used together, reflecting the interconnectedness of these fields.

The top words analysis from Topic 1, Topic 2, and Topic 3 reveal that conversations on the platform are predominantly positive and focused on various aspects of Machine Learning, Artificial Intelligence, and Data Science. This further supports the findings from the sentiment analysis models, emphasizing the increasing interest and engagement in these areas over the years.

## Implications for Machine Learning community

The implications for the Machine Learning community based on the analysis are as follows:

1.  Growing interest and engagement: The increasing post activity and positive sentiment indicate that more people are joining the community and actively participating in discussions. This can lead to a more diverse and inclusive environment, fostering collaboration and innovation.

2.  Expanding knowledge base: The upward trend in the number of distinct words per post suggests that conversations are becoming more in-depth and covering a wider range of topics. This can help expand the collective knowledge base and facilitate the development of new ideas and techniques in Machine Learning.

3.  Focus on applications and impact: The increasing anticipation in the 'Career' category and the interconnectedness of common words related to data science, artificial intelligence, and deep learning highlight the growing awareness of the real-world applications and implications of Machine Learning. The community can leverage this understanding to address potential challenges and explore new opportunities across various industries.

4.  Need for continuous learning and adaptation: As Machine Learning continues to evolve and integrate with other fields, it is essential for community members to stay up-to-date with the latest research and advancements. This will help them adapt to the changing landscape and ensure that their skills remain relevant in the job market.

5.  Importance of addressing concerns and misconceptions: The negative polarity trend observed in some cases underscores the need for the Machine Learning community to address concerns and misconceptions surrounding the technology. By engaging in open and honest discussions, the community can build trust and promote a better understanding of Machine Learning's potential benefits and limitations.

6.  Collaboration and interdisciplinary research: The synergistic relationship between data science, artificial intelligence, and deep learning emphasizes the importance of collaboration and interdisciplinary research. By working together and drawing from diverse perspectives, the Machine Learning community can drive innovation and unlock new potential applications.

## Limitations of the study

There are several possible limitations of the study that should be considered:

1.  Data source: The study may have relied on data from a single platform or a limited number of sources. This could result in a biased representation of the Machine Learning community and its discussions. Including data from a wider range of sources and platforms could provide a more comprehensive view.

2.  Temporal limitations: The study's data may only cover a specific period, potentially missing out on more recent developments or trends in the Machine Learning community. Continuous updates to the data and analysis would be necessary to maintain relevance and accuracy.

3.  Sentiment analysis limitations: Sentiment analysis is often based on algorithms that may not accurately capture the nuances and complexities of human emotions and language. Sarcasm, irony, and context can be challenging for these algorithms to interpret, which could result in misclassification of sentiment.

4.  Topic modeling limitations: The study's topic modeling approach might not perfectly capture the underlying themes and topics discussed within the community. The choice of parameters, the number of topics, and the algorithm itself could influence the results, leading to a different representation of the data.

5.  Limited scope: The study might focus on a specific aspect of Machine Learning, Artificial Intelligence, or Data Science, potentially overlooking other important factors or areas of interest. A broader scope could provide a more comprehensive understanding of the community and its discussions.

6.  Generalizability: The findings of the study might not be generalizable to other communities or fields. Differences in culture, language, and context could limit the applicability of the results to other areas.

7.  Potential biases: The study might be subject to biases, such as selection bias, confirmation bias, or researcher bias. These biases could influence the interpretation of the data and the conclusions drawn from the analysis.

To address these limitations, future research could incorporate data from diverse sources, continuously update the dataset, refine sentiment and topic modeling approaches, broaden the scope of the study, and rigorously assess potential biases to provide a more accurate and comprehensive understanding of the Machine Learning community and its discussions.

# Conclusion

In conclusion, the analysis of the various figures and sentiments demonstrates a significant growth in interest and engagement in the fields of Machine Learning, Artificial Intelligence, and Data Science over the years. The increase in post activity, distinct words per post, and predominantly positive sentiment reveal a deeper understanding and appreciation of these technologies and their potential applications.

The interconnectedness of the common words related to these fields underscores their synergistic relationship, which will likely continue to drive further advancements and breakthroughs. Moreover, the increase in anticipation within the 'Career' category suggests that people are both excited and concerned about the potential impact of these technologies on the job market.

The growing interest and engagement in Machine Learning, Artificial Intelligence, and Data Science reflect the recognition of their importance in shaping the future of various industries. As these technologies continue to advance, we can expect even more in-depth and diverse conversations, fostering greater understanding, collaboration, and innovation in these areas.

# References
